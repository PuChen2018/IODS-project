---
title: "Dhapter 4"
author: "Pu Chen"
date: "11/28/2021"
output: html_document
---
*1. Here I loaded the dataset "Boston" into R. It is a dataset that include 16 varibles of 506 observations. It reavel the housing value in boston and other variables that related to the different housing. Varible medv indicate the median value of the house, it is negatively correlate with lower status of the population (percent) and proportion of non-retail business acres per town and postively correlated with average number of rooms per dwelling.*
```{r}
library(MASS)
library(tidyr)
library(corrplot)
library(GGally)
data("Boston")
#explore the struction of the data set 
str(Boston)
summary(Boston)
p <- ggpairs(Boston, lower = list(combo = wrap("facethist", bins = 20)))
p
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)
# print the correlation matrix
cor_matrix
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
```
*2.transformed the dataset to a standardized dataframe and add a new column indicate the crime numbers in a categorical manners-low, med-low, med-high, high.And subset the data to a training set and test set. *
```{r}
# center and standardize variables
boston_scaled <- scale(Boston)
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
# summary of the scaled crime rate
summary(boston_scaled$crim)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
# look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
n
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set 
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```
*3. Fit the linear discriminant analysis on the train set. Here used the crime as the target variables and others as a predicator variables.*
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)


```
*4.use the LDA to predict the crime category and then compare with the actual category. The prediction of for the high crime group is the most accurate group.*
```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
*5. caculating the dsitance 
```{r}
data("Boston")
boston_scaled_k <- scale(Boston)
summary(boston_scaled_k)
# euclidean distance matrix
dist_eu <- dist(boston_scaled_k)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method = 'manhattan')

# look at the summary of the distances
summary(dist_man)

km <-kmeans(Boston, centers = 3)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

# Boston dataset is available
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
library(ggplot2)
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)